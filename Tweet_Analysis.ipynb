{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import MeCab\n",
    "from decimal import *\n",
    "tagger = MeCab.Tagger (\"-Ochasen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print([user.ScreenName for user in users])\n",
    "def get_follow_users(screen_name,output_tsv_file='follow_users.tsv'):\n",
    "    follow_users= [[u.name,u.screen_name,u.description,u.followers_count,u.statuses_count,u.protected] for u in api.GetFriends(screen_name=screen_name)]\n",
    "    follow_users_df = pd.DataFrame(follow_users,columns=[\"name\",\"screen_name\",\"description\",\"followers_count\",\"statuses_count\",\"is_protected\"])\n",
    "    follow_users_df.to_csv(output_tsv_file,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(screen_name,num_of_tweet=100):\n",
    "    return api.GetUserTimeline(screen_name=screen_name,count=num_of_tweet) #Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_follow_users_list(input_tsv_file=\"follow_users.tsv\",is_exclude_protected=True):\n",
    "    df = pd.read_csv(input_tsv_file,sep='\\t')\n",
    "    #非公開ユーザーを除外\n",
    "    if is_exclude_protected:\n",
    "        df = df[df[\"is_protected\"] == 0]\n",
    "    #listで返却\n",
    "    return df.screen_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_followers_list(followers,output_file_name,is_exclude_protected=True):\n",
    "    output = []\n",
    "    for fw in followers:\n",
    "        if is_exclude_protected:\n",
    "            if fw.protected == False:\n",
    "                #商用ツイッター（公式と入っているユーザー）やフォローしている相手が千人を超えるユーザーは除く\n",
    "                if (fw.description.find('公式') == -1) and (fw.friends_count < 1000):\n",
    "                    output.append(fw.screen_name)\n",
    "    pd.DataFrame(output).to_csv(output_file_name,sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_from_csv(input_tsv_file):\n",
    "    #フォローリストからツイートを取得\n",
    "    tweets = []\n",
    "    f_list = pd.read_csv(input_tsv_file,sep='\\t')\n",
    "    for screen_name in f_list:\n",
    "        tweets.append(get_tweets(screen_name))\n",
    "    tw = []\n",
    "    for user_tweets in tweets:\n",
    "        tw.append([[t.user.name,t.user.description,t.text,t.created_at] for t in user_tweets])\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文章（ツイート）を分かち書きして言葉と品詞の二次元配列を得る\n",
    "# sentence例 'ニンジンサラダのレシピ試してみた。これは本当に美味しい'\n",
    "def get_parsed_sentence(sentence):\n",
    "    ret = []\n",
    "    sentence = tagger.parse(sentence)\n",
    "    for line in sentence.splitlines():\n",
    "        feature = line.split('\\t')\n",
    "    #     print(feature)\n",
    "        if feature[0] != 'EOS': \n",
    "            tmp = []\n",
    "            tmp.append(feature[0])\n",
    "            tmp.append(feature[3])\n",
    "        ret.append(tmp)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set keys\n",
    "CK = ''\n",
    "CS = ''\n",
    "AK = ''\n",
    "AS = ''\n",
    "api = tw.Api(consumer_key=CK,\n",
    "                      consumer_secret=CS,\n",
    "                      access_token_key=AK,\n",
    "                      access_token_secret=AS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#上限1000人に設定。15分に１５回アクセス制限があるため、１度に200ユーザーを５回取得するだけにとどめる\n",
    "TGT_SCREEN_NAME = ''\n",
    "TGT_FOLLOWERS = api.GetFollowers(screen_name=TGT_SCREEN_NAME,count=200,total_count=2000)\n",
    "FOLLOWERS_FILE_NAME = \"tsv/tgt_followers.tsv\"\n",
    "export_followers_list(TGT_FOLLOWERS,FOLLOWERS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#ツイート取得\n",
    "f_df = pd.read_csv(FOLLOWERS_FILE_NAME,sep='\\t')\n",
    "f_list = f_df.as_matrix()\n",
    "tweets = []\n",
    "tw = []\n",
    "#ツイート取得制限は15分で900回まで\n",
    "for s_name in f_list:\n",
    "    tweets = []\n",
    "    tweets.append(get_tweets(s_name[0],50))\n",
    "    for user_tweets in tweets:\n",
    "        tw.append([[t.user.name,t.user.description,t.text,t.created_at] for t in user_tweets])\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#取得ツイートをtsvで出力\n",
    "tw_df  = pd.DataFrame(columns=['screen_name','desciption','text','created_at'])\n",
    "for t in tw:\n",
    "    tmp = pd.DataFrame(t,columns=['screen_name','desciption','text','created_at'])\n",
    "    tw_df  =pd.concat([tw_df,tmp])\n",
    "tw_df.text = tw_df.text.str.replace('\\n','')\n",
    "tw_df.text = tw_df.text.str.replace('\\t','')\n",
    "tw_df.to_csv('tsv/tweet.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#名詞辞書の取得\n",
    "noun_dic = {}\n",
    "#reference:Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji Tateishi. Collecting Evaluative Expressions for Opinion Extraction, Journal of Natural Language Processing 12(3), 203-222, 2005.\n",
    "with open(\"./dic/pn.csv.m3.120408.trim\") as f:\n",
    "    for line in f:\n",
    "       (key, feeling,prop) = line.rstrip('\\n').split('\\t')\n",
    "       noun_dic[key] = feeling\n",
    "#用言辞書の取得\n",
    "#reference:Masahiko Higashiyama, Kentaro Inui, Yuji Matsumoto. Learning Sentiment of Nouns from Selectional Preferences of Verbs and Adjectives, Proceedings of the 14th Annual Meeting of the Association for Natural Language Processing, pp.584-587, 2008.\n",
    "dec_dic = {}\n",
    "with open(\"./dic/wago.121808.pn\") as f:\n",
    "    for line in f:\n",
    "        (feeling,key) = line.rstrip('\\n').split('\\t')\n",
    "        dec_dic[key] = feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#評価リストを作成\n",
    "tw_df = pd.read_csv('tsv/tweet.tsv',sep='\\t')\n",
    "tw_df = tw_df[tw_df.text.notnull()]\n",
    "tgts = np.c_[tw_df.screen_name.as_matrix(), tw_df.desciption.as_matrix(), tw_df.text.as_matrix()]\n",
    "total = 0\n",
    "eva_list = []\n",
    "for tgt_sent in tgts:\n",
    "    ret = get_parsed_sentence(tgt_sent[2])\n",
    "    for word in ret:\n",
    "        tmp = []\n",
    "        if '名詞' in word[1] and word[0] in noun_dic:\n",
    "            tmp.append(tgt_sent[0])\n",
    "            tmp.append(tgt_sent[1])\n",
    "            tmp.append(word[0])\n",
    "            tmp.append(noun_dic[word[0]])\n",
    "            tmp.append('名詞')\n",
    "            eva_list.append(tmp)\n",
    "        elif ('形容詞' or '副詞' or '動詞' in word[1]) and word[0] in dec_dic:\n",
    "            tmp.append(tgt_sent[0])\n",
    "            tmp.append(tgt_sent[1])\n",
    "            tmp.append(word[0])\n",
    "            tmp.append(dec_dic[word[0]])\n",
    "            tmp.append('用言')\n",
    "            eva_list.append(tmp)\n",
    "df = pd.DataFrame(eva_list,columns=['screen_name','description','word','feeling','type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['positive_flg'] = ((df['feeling'].str.contains('ポジ')) | (df['feeling'].str.contains('p'))).astype(int)\n",
    "df['negative_flg'] = ((df['feeling'].str.contains('ネガ')) | (df['feeling'].str.contains('n'))).astype(int)\n",
    "df['eva'] = df['positive_flg'] + df['negative_flg'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分かち書きスコアつきツイート出力\n",
    "df.to_csv('tsv/evaluated_tweet.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ユーザースコア\n",
    "grp_df = df.groupby('screen_name')['eva'].mean()\n",
    "grp_df.to_csv('tsv/grp_evaluated_tweet.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eva_df = pd.read_csv(\"tsv/evaluated_tweet.tsv\",sep='\\t')\n",
    "grp_df = pd.read_csv(\"tsv/grp_evaluated_tweet.tsv\",sep='\\t',names=(\"screen_name\",\"score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eva_score = eva_df.groupby('screen_name',as_index=False)['eva'].mean()\n",
    "eva_score.rename(columns={'eva':'score'})\n",
    "eva_word_count = eva_df.groupby(['screen_name','word'],as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eva_word_count = eva_df.groupby(['screen_name','word','positive_flg','negative_flg'],as_index=False).count()\n",
    "eva_word_count.rename(columns={'Unnamed: 0':'cnt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ユーザーごとのスコアをマージ\n",
    "mrg_df = pd.merge(eva_df, grp_df,on=\"screen_name\",how=\"left\",left_index=False,right_index=False,sort=True)\n",
    "mrg_df = pd.merge(mrg_df, eva_score,on=\"screen_name\",how=\"left\",left_index=False,right_index=False,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df.to_csv(\"tsv/merge_eva_tweet.tsv\",sep='\\t')\n",
    "eva_word_count.to_csv(\"tsv/eva_word_count.tsv\",sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
